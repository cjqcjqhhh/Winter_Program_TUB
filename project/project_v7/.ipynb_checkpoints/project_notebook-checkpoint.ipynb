{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product recommendation engine\n",
    "---\n",
    "![](resources/groceries.jpg)\n",
    "\n",
    "## Main objective:\n",
    "---\n",
    "Imagine we are a grocery store owner, and we are trying to maximize the purchases of our customers per visit. \n",
    "\n",
    "A first strategy that comes to our mind is placing products next to each other that are usually bought together.\n",
    "\n",
    "Since we have succesfully completed a Data Science task in the past we immediately realize that this problem can be formulated as a recommendation task.\n",
    "\n",
    "\n",
    "The machine learning part has the following goal:\n",
    "\n",
    "\n",
    "Essentially we will try to predict the last item of a customers purchase list, given all the other items that he has already in his shopping basket. Those predictions are a helpful first heuristic for the placement of certain products in our grocery store. \n",
    "\n",
    "Thus we start collecting the purchase histories of past customers and start writing down the following steps needed, to build our recommendation pipeline:\n",
    "\n",
    "\n",
    "### Plan of attack:\n",
    "1. Load the customer purchase data, located in 'data/training_data.csv', 'data/training_labels.csv'\n",
    "    - Note on the dataset: Each row in each of the data files refers to one 'incomplete' item-list of a customers purchase.\n",
    "    - The labels represent the item that was purchased by the customer in addition to the items in the dataset\n",
    "    \n",
    "    \n",
    "2. Plot the following statistics:\n",
    "    - histogram of 10 most purchased products\n",
    "    - pie chart of all product purchase frequencies\n",
    "    - which other interesting plots can you think of ? -> extra points\n",
    "\n",
    "\n",
    "\n",
    "3. Compute and present the following results(you are free to choose any method to present your results):\n",
    "    - Find the pair of products, that are bought together the most\n",
    "    - How many customers purchased all the products \n",
    "    - Which product was the least purchased ?\n",
    "\n",
    "\n",
    "4. Transform it into a Machine learning-classifier digestable format:\n",
    "    - Machine learning algorithms consume data, that has a unified format!\n",
    "    - For example it should look like that:\n",
    "    \n",
    "    \n",
    "    | feature 1(e.g. product/grocery): | feature 2: | ... | feature N: |\n",
    "    | \"apple\"                          | \"banana\"   | ... | mango      |\n",
    "    --------------------------------------------------------------------\n",
    "    | no                               | yes        | ... | no         | <- customer 1: purchased only banana \n",
    "    --------------------------------------------------------------------\n",
    "    | yes                              | yes        | ... | yes        | <- customer 2: purchased all 3 shown\n",
    "    -------------------------------------------------------------------- \n",
    "                                .\n",
    "                                .\n",
    "                                .\n",
    "    --------------------------------------------------------------------\n",
    "    | no                              | no         | ... | no          | <- customer N: purchased nothing\n",
    "    --------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "\n",
    "5. Train your model on the training set, and predict an item for the each row in the test set(DON'T change the order of the test set):\n",
    "    - Item-predictions should be in the original string format(=item name)\n",
    "\n",
    "\n",
    "\n",
    "6. Save the predictions for the test set in a csv-file\n",
    "\n",
    "\n",
    "### Note on implementation:\n",
    "- You are free to use any classification algorithm that you want. If you find better recommendation approaches on the web(there certainly are better, but also more involved ones), you are free to use those. The main goal though will be to \n",
    "- Try to implement classes\n",
    "\n",
    "\n",
    "### Note on grading:\n",
    "- End result = 25%\n",
    "- Clean code(e.g. classes instead of script like functions etc.) = 25 %\n",
    "- Documentation = 25%\n",
    "- Usage of numpy, pandas, pyplot etc. functions for faster computation = 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\11518\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#all modules used\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'png')\n",
    "plt.rcParams['savefig.dpi'] = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('data/training_data.csv', header=None)\n",
    "test_x = pd.read_csv('data/test_data.csv', header=None)\n",
    "train_y = pd.read_csv('data/training_labels.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductRcmd:\n",
    "    \n",
    "    def __init__(self, df_items, df_labels):\n",
    "        self.rawdata = df_items\n",
    "        self.rawlabels = df_labels\n",
    "        \n",
    "    def Data_preproc(self, NAdrop=True, onehot=True, fwrite=True,\\\n",
    "                     dataname=\"new_data.csv\", labelsname=\"new_labels.csv\"):\n",
    "        if NAdrop:\n",
    "            self.rawdata.dropna(axis=0, how='all', inplace=True)\n",
    "        if onehot:\n",
    "            self.data = pd.get_dummies(self.rawdata, prefix='').groupby(axis = 1, level = 0).sum()\n",
    "            self.data.columns = self.data.columns.str.replace(\"_\", \"\")\n",
    "            \n",
    "        self.ItemType = self.data.shape[1]\n",
    "        self.CustNum = self.data.shape[0]\n",
    "        \n",
    "        self.data.index = [\"Custom No.\" + str(index) for index in range(self.CustNum)]\n",
    "        self.labels = self.rawlabels.iloc[self.rawdata.index,:]\n",
    "        self.LabelType = np.unique(self.labels).shape[0]\n",
    "        self.labels.index = [\"Custom No.\" + str(index) for index in range(self.CustNum)]\n",
    "        if fwrite:\n",
    "            self.data.to_csv(dataname)\n",
    "            self.labels.to_csv(labelsname)\n",
    "            \n",
    "    def Data_split(self, train_size=0.8, shuffle=False):\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y =\\\n",
    "        train_test_split(self.data, self.labels, train_size = train_size, shuffle=shuffle)\n",
    "        \n",
    "    #def PieChart(self, sort=False, Num=10):\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRcmd = ProductRcmd(train_x, train_y)\n",
    "myRcmd.Data_preproc()\n",
    "myRcmd.Data_split(shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Custom No.3389</th>\n",
       "      <td>frozen vegetables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom No.4109</th>\n",
       "      <td>turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom No.610</th>\n",
       "      <td>burgers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom No.2414</th>\n",
       "      <td>french fries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom No.3712</th>\n",
       "      <td>cookies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom No.2948</th>\n",
       "      <td>herb &amp; pepper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom No.4468</th>\n",
       "      <td>turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom No.1328</th>\n",
       "      <td>mineral water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom No.4268</th>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom No.3052</th>\n",
       "      <td>burgers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4136 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0\n",
       "Custom No.3389  frozen vegetables\n",
       "Custom No.4109             turkey\n",
       "Custom No.610             burgers\n",
       "Custom No.2414       french fries\n",
       "Custom No.3712            cookies\n",
       "...                           ...\n",
       "Custom No.2948      herb & pepper\n",
       "Custom No.4468             turkey\n",
       "Custom No.1328      mineral water\n",
       "Custom No.4268               milk\n",
       "Custom No.3052            burgers\n",
       "\n",
       "[4136 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pre-handle, remove meaningless lines, add columns and index, onehot coding\n",
    "data = train_x\n",
    "data.dropna(axis=0, how='all', inplace=True)\n",
    "new_data = pd.get_dummies(data, prefix='')\n",
    "nn_data = new_data.groupby(axis = 1, level = 0).sum()\n",
    "nn_data.columns = nn_data.columns.str.replace(\"_\", \"\")\n",
    "nn_data.index = [\"Custom No.\" + str(index) for index in range(5171)]\n",
    "nn_data.to_csv(\"new_data.csv\")\n",
    "labels = train_y.iloc[data.index,:]\n",
    "labels.index = [\"Custom No.\" + str(index) for index in range(5171)]\n",
    "labels.to_csv(\"new_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#piechart plot\n",
    "mpie = nn_data.apply(sum)[:10]\n",
    "plt.pie(x=mpie.values, labels=mpie.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historgram\n",
    "number = nn_data.apply(sum).sort_values(ascending = False)[:10]\n",
    "plt.bar([0,1,2,3,4,5,6,7,8,9], number.values)\n",
    "plt.xticks([0,1,2,3,4,5,6,7,8,9], number.index, rotation='90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old version get best pair, slow, provide less info\n",
    "def OldGetPair(nn_data):\n",
    "    adata = np.asarray(nn_data)*0.5\n",
    "    best_comb = 0\n",
    "    best = [[]]\n",
    "    for sft in range(59):\n",
    "        ndata = np.hstack((adata[:,sft+1:], adata[:,:sft+1]))\n",
    "        ndata = ((np.floor(ndata + adata)).sum(axis=0))\n",
    "        dmax = ndata.max()\n",
    "        if best_comb < dmax:\n",
    "            best_comb = dmax\n",
    "            best = [[ndata.argmax(), sft+1]]\n",
    "        elif best_comb == dmax:\n",
    "            best.append([[ndata.argmax(), sft+1]])\n",
    "    return best\n",
    "\n",
    "best = np.asarray(OldGetPair(nn_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new version faster\n",
    "def GetPairMap(nn_data):\n",
    "    adata = np.asarray(nn_data)*0.5\n",
    "    pairs = np.ones([119,119])\n",
    "    order0 = list(range(119))\n",
    "    for sft in range(1,119):\n",
    "        ndata = np.hstack((adata[:,sft:], adata[:,:sft]))\n",
    "        order1 = list(range(sft,119)) + list(range(sft))\n",
    "        ndata = ((np.floor(ndata + adata)).sum(axis=0))\n",
    "        pairs[order0,order1] = ndata\n",
    "    return np.array(pairs)\n",
    "\n",
    "PairMap = GetPairMap(nn_data)\n",
    "mask = np.asarray([PairMap == PairMap.max()])\n",
    "\n",
    "mask[0].nonzero()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heat map for connection \n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "ax = sns.heatmap(PairMap, vmax=305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#least bought item\n",
    "least_item = nn_data.sum(axis=0).idxmin()\n",
    "least_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

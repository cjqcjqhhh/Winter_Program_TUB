{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product recommendation engine\n",
    "---\n",
    "![](resources/groceries.jpg)\n",
    "\n",
    "## Main objective:\n",
    "---\n",
    "Imagine we are a grocery store owner, and we are trying to maximize the purchases of our customers per visit. \n",
    "\n",
    "A first strategy that comes to our mind is placing products next to each other that are usually bought together.\n",
    "\n",
    "Since we have succesfully completed a Data Science task in the past we immediately realize that this problem can be formulated as a recommendation task.\n",
    "\n",
    "\n",
    "The machine learning part has the following goal:\n",
    "\n",
    "\n",
    "Essentially we will try to predict the last item of a customers purchase list, given all the other items that he has already in his shopping basket. Those predictions are a helpful first heuristic for the placement of certain products in our grocery store. \n",
    "\n",
    "Thus we start collecting the purchase histories of past customers and start writing down the following steps needed, to build our recommendation pipeline:\n",
    "\n",
    "\n",
    "### Plan of attack:\n",
    "1. Load the customer purchase data, located in 'data/training_data.csv', 'data/training_labels.csv'\n",
    "    - Note on the dataset: Each row in each of the data files refers to one 'incomplete' item-list of a customers purchase.\n",
    "    - The labels represent the item that was purchased by the customer in addition to the items in the dataset\n",
    "    \n",
    "    \n",
    "2. Plot the following statistics:\n",
    "    - histogram of 10 most purchased products\n",
    "    - pie chart of all product purchase frequencies\n",
    "    - which other interesting plots can you think of ? -> extra points\n",
    "\n",
    "\n",
    "\n",
    "3. Compute and present the following results(you are free to choose any method to present your results):\n",
    "    - Find the pair of products, that are bought together the most\n",
    "    - How many customers purchased all the products \n",
    "    - Which product was the least purchased ?\n",
    "\n",
    "\n",
    "4. Transform it into a Machine learning-classifier digestable format:\n",
    "    - Machine learning algorithms consume data, that has a unified format!\n",
    "    - For example it should look like that:\n",
    "    \n",
    "    \n",
    "    | feature 1(e.g. product/grocery): | feature 2: | ... | feature N: |\n",
    "    | \"apple\"                          | \"banana\"   | ... | mango      |\n",
    "    --------------------------------------------------------------------\n",
    "    | no                               | yes        | ... | no         | <- customer 1: purchased only banana \n",
    "    --------------------------------------------------------------------\n",
    "    | yes                              | yes        | ... | yes        | <- customer 2: purchased all 3 shown\n",
    "    -------------------------------------------------------------------- \n",
    "                                .\n",
    "                                .\n",
    "                                .\n",
    "    --------------------------------------------------------------------\n",
    "    | no                              | no         | ... | no          | <- customer N: purchased nothing\n",
    "    --------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "\n",
    "5. Train your model on the training set, and predict an item for the each row in the test set(DON'T change the order of the test set):\n",
    "    - Item-predictions should be in the original string format(=item name)\n",
    "\n",
    "\n",
    "\n",
    "6. Save the predictions for the test set in a csv-file\n",
    "\n",
    "\n",
    "### Note on implementation:\n",
    "- You are free to use any classification algorithm that you want. If you find better recommendation approaches on the web(there certainly are better, but also more involved ones), you are free to use those. The main goal though will be to \n",
    "- Try to implement classes\n",
    "\n",
    "\n",
    "### Note on grading:\n",
    "- End result = 25%\n",
    "- Clean code(e.g. classes instead of script like functions etc.) = 25 %\n",
    "- Documentation = 25%\n",
    "- Usage of numpy, pandas, pyplot etc. functions for faster computation = 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\apple\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#all modules used\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'png')\n",
    "plt.rcParams['savefig.dpi'] = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('data/training_data.csv', header=None)\n",
    "test_x = pd.read_csv('data/test_data.csv', header=None)\n",
    "train_y = pd.read_csv('data/training_labels.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductRcmd:\n",
    "    \n",
    "    def __init__(self, df_items, df_labels):\n",
    "        self.rawdata = df_items\n",
    "        self.rawlabels = df_labels\n",
    "        \n",
    "    def Data_preproc(self, NAdrop=True, onehot=True, fwrite=True,\\\n",
    "                     dataname=\"new_data.csv\", labelsname=\"new_labels.csv\"):\n",
    "        if NAdrop:\n",
    "            self.rawdata.dropna(axis=0, how='all', inplace=True)\n",
    "        if onehot:\n",
    "            self.data = pd.get_dummies(self.rawdata, prefix='').groupby(axis = 1, level = 0).sum()\n",
    "            self.data.columns = self.data.columns.str.replace(\"_\", \"\")\n",
    "            \n",
    "        self.ItemType = self.data.shape[1]\n",
    "        self.CustNum = self.data.shape[0]\n",
    "        \n",
    "        self.data.index = [\"Custom No.\" + str(index) for index in range(self.CustNum)]\n",
    "        self.labels = self.rawlabels.iloc[self.rawdata.index,:]\n",
    "        self.LabelType = np.unique(self.labels).shape[0]\n",
    "        self.labels.index = [\"Custom No.\" + str(index) for index in range(self.CustNum)]\n",
    "        if fwrite:\n",
    "            self.data.to_csv(dataname)\n",
    "            self.labels.to_csv(labelsname)\n",
    "            \n",
    "    def Data_split(self, train_size=0.8, shuffle=False):\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y =\\\n",
    "        train_test_split(self.data, self.labels, train_size = train_size, shuffle=shuffle)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRcmd = ProductRcmd(train_x, train_y)\n",
    "myRcmd.Data_preproc()\n",
    "myRcmd.Data_split(shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4136, 1), (4136, 119))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRcmd.train_y.shape,myRcmd.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pre-handle, remove meaningless lines, add columns and index, onehot coding\n",
    "data = train_x\n",
    "data.dropna(axis=0, how='all', inplace=True)\n",
    "new_data = pd.get_dummies(data, prefix='')\n",
    "nn_data = new_data.groupby(axis = 1, level = 0).sum()\n",
    "nn_data.columns = nn_data.columns.str.replace(\"_\", \"\")\n",
    "nn_data.index = [\"Custom No.\" + str(index) for index in range(5171)]\n",
    "nn_data.to_csv(\"new_data.csv\")\n",
    "labels = train_y.iloc[data.index,:]\n",
    "labels.index = [\"Custom No.\" + str(index) for index in range(5171)]\n",
    "labels.to_csv(\"new_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Neural Network\n",
    "稍微改了一下，用train data训练用test data测试，把layer改成2层，结果同样是7%。\n",
    "问就是不知道为什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = myRcmd.train_x\n",
    "test_x = myRcmd.test_x\n",
    "train_y = pd.get_dummies(myRcmd.train_y)\n",
    "test_y = pd.get_dummies(myRcmd.test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "\n",
    "###### GET THE TRAIN TARGET UNIQUE LIST #####\n",
    "def Get_train_targets(train_y):\n",
    "    train_y_dummies = pd.get_dummies(list(train_y[0]))\n",
    "\n",
    "    # get a dictionary\n",
    "    product_dict = {}\n",
    "    for i,product in enumerate(nn_data.columns):\n",
    "        product_dict[product] = i\n",
    "\n",
    "    #rename the dummies\n",
    "    train_y_idx = train_y_dummies.rename(columns=product_dict)\n",
    "\n",
    "    #return the idxmax value\n",
    "    train_targets = train_y_idx.idxmax(axis=1)\n",
    "    \n",
    "    return train_targets\n",
    "\n",
    "#### GET THE TRAINING DATAPOINT #####\n",
    "def Get_train_datas(train_x):\n",
    "    #### receive one hot data!!!! #####\n",
    "    one_hot_training_data = train_x\n",
    "    return one_hot_training_data\n",
    "\n",
    "class CustomDataset():\n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "\n",
    "        self.target = torch.LongTensor(labels)\n",
    "        self.data = torch.Tensor(np.asarray(data))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]\n",
    "    \n",
    "#### BATCH THE DATAPOINT ####\n",
    "def Get_batch(one_hot_training_data, train_targets, size):\n",
    "    # zip the data and target\n",
    "    training_data = CustomDataset(one_hot_training_data,train_targets)\n",
    "    # batch the data point\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size=size, shuffle=True)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        \n",
    "        super(Neural_Network, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(self.input_dim, 10)\n",
    "        \n",
    "        self.layer2 = torch.nn.Linear(10, self.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x.view(-1, self.input_dim))\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = Neural_Network(119, 119)\n",
    "optimizer = torch.optim.SGD(params=neural_net.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2e133860bce3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# set optimizer gradients to zero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# training loop:\n",
    "\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader, 1):\n",
    "        \n",
    "        # set optimizer gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        predictions = neural_net.forward(x)\n",
    "                \n",
    "        # backward pass + optimization step\n",
    "        loss = loss_fn(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f'Epoch: {epoch}, loss: {running_loss / i}')\n",
    "        \n",
    "    print(f'Loss after epoch: {epoch} = {running_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        outputs = neural_net(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_targets(train_y):\n",
    "    train_y_dummies = pd.get_dummies(list(train_y[0]))\n",
    "\n",
    "    # get a dictionary\n",
    "    product_dict = {}\n",
    "    for i,product in enumerate(nn_data.columns):\n",
    "        product_dict[product] = i\n",
    "\n",
    "    #rename the dummies\n",
    "    train_y_idx = train_y_dummies.rename(columns=product_dict)\n",
    "\n",
    "    #return the idxmax value\n",
    "    train_targets = train_y_idx.idxmax(axis=1)\n",
    "    \n",
    "    return train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = myRcmd.train_x\n",
    "test_x = myRcmd.test_x\n",
    "train_y = Get_targets(myRcmd.train_y)\n",
    "test_y = Get_targets(myRcmd.test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(np.asarray(train_x), np.asarray(train_y).reshape(-1,1))\n",
    "clf.decision_function_shape = \"ovr\"\n",
    "dec = clf.decision_function(np.asarray(myRcmd.test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_targets(train_y):\n",
    "    train_y_dummies = pd.get_dummies(list(train_y[0]))\n",
    "\n",
    "    # get a dictionary\n",
    "    product_dict = {}\n",
    "    for i,product in enumerate(nn_data.columns):\n",
    "        product_dict[product] = i\n",
    "\n",
    "    #rename the dummies\n",
    "    train_y_idx = train_y_dummies.rename(columns=product_dict)\n",
    "\n",
    "    #return the idxmax value\n",
    "    train_targets = train_y_idx.idxmax(axis=1)\n",
    "    \n",
    "    return train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = myRcmd.train_x\n",
    "test_x = myRcmd.test_x\n",
    "train_y = Get_targets(myRcmd.train_y)\n",
    "test_y = Get_targets(myRcmd.test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "clf = SVR(C=500, epsilon=0.8)\n",
    "clf.fit(train_x, train_y)\n",
    "predict_y = clf.predict(test_x).astype(int)\n",
    "list(predict_y), test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for i, num in enumerate(list(predict_y)):\n",
    "    if list(predict_y)[i] == test_y[i]:\n",
    "        k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

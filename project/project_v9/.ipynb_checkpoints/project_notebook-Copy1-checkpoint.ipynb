{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Another methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Logistic_Reg(X,Y)\n",
    "Implement logistic regression to training data.  \n",
    "**<font>Parameters:</font>**  \n",
    "X: dataframe  \n",
    "    &emsp; The training data points  \n",
    "Y: dataframe  \n",
    "    &emsp; The training data labels   \n",
    "\n",
    "**<font>Return:</font>**   \n",
    "accuracy: float  \n",
    "    &emsp; The accuracy of the training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Reg(X,Y):\n",
    "    clf = LogisticRegression(random_state=0).fit(X, Y)\n",
    "    accuracy = (clf.predict(myData.test_x) == ans).sum()/len(ans)\n",
    "    return accuracy\n",
    "\n",
    "Logistic_Reg(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nerual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font>This is an old version. Due to the difference of naming, I copied most of the necessary part of its structureher. It may be a bit long.  \n",
    "Just list here for fun. :D</font>**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductRcmd:\n",
    "    \n",
    "    def __init__(self, df_items, df_labels):\n",
    "        self.rawdata = df_items\n",
    "        self.rawlabels = df_labels\n",
    "        \n",
    "    def Data_preproc(self, NAdrop=True, onehot=True, fwrite=True,\\\n",
    "                     dataname=\"new_data.csv\", labelsname=\"new_labels.csv\"):\n",
    "        if NAdrop:\n",
    "            self.rawdata.dropna(axis=0, how='all', inplace=True)\n",
    "        if onehot:\n",
    "            self.data = pd.get_dummies(self.rawdata, prefix='').groupby(axis = 1, level = 0).sum()\n",
    "            self.data.columns = self.data.columns.str.replace(\"_\", \"\")\n",
    "            \n",
    "        self.ItemType = self.data.shape[1]\n",
    "        self.CustNum = self.data.shape[0]\n",
    "        \n",
    "        self.data.index = [\"Custom No.\" + str(index) for index in range(self.CustNum)]\n",
    "        self.labels = self.rawlabels.iloc[self.rawdata.index,:]\n",
    "        self.LabelType = np.unique(self.labels).shape[0]\n",
    "        self.labels.index = [\"Custom No.\" + str(index) for index in range(self.CustNum)]\n",
    "        if fwrite:\n",
    "            self.data.to_csv(dataname)\n",
    "            self.labels.to_csv(labelsname)\n",
    "            \n",
    "    def Data_split(self, train_size=0.8, shuffle=False):\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y =\\\n",
    "        train_test_split(self.data, self.labels, train_size = train_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRcmd = ProductRcmd(train_x, train_y)\n",
    "myRcmd.Data_preproc()\n",
    "myRcmd.Data_split(shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-handle, remove meaningless lines, add columns and index, onehot coding\n",
    "data = train_x\n",
    "data.dropna(axis=0, how='all', inplace=True)\n",
    "new_data = pd.get_dummies(data, prefix='')\n",
    "nn_data = new_data.groupby(axis = 1, level = 0).sum()\n",
    "nn_data.columns = nn_data.columns.str.replace(\"_\", \"\")\n",
    "nn_data.index = [\"Custom No.\" + str(index) for index in range(5171)]\n",
    "nn_data.to_csv(\"new_data.csv\")\n",
    "labels = train_y.iloc[data.index,:]\n",
    "labels.index = [\"Custom No.\" + str(index) for index in range(5171)]\n",
    "labels.to_csv(\"new_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = myRcmd.train_x\n",
    "test_x = myRcmd.test_x\n",
    "train_y = pd.get_dummies(myRcmd.train_y)\n",
    "test_y = pd.get_dummies(myRcmd.test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "\n",
    "###### GET THE TRAIN TARGET UNIQUE LIST #####\n",
    "def Get_train_targets(train_y):\n",
    "    train_y_dummies = pd.get_dummies(list(train_y[0]))\n",
    "\n",
    "    # get a dictionary\n",
    "    product_dict = {}\n",
    "    for i,product in enumerate(nn_data.columns):\n",
    "        product_dict[product] = i\n",
    "\n",
    "    #rename the dummies\n",
    "    train_y_idx = train_y_dummies.rename(columns=product_dict)\n",
    "\n",
    "    #return the idxmax value\n",
    "    train_targets = train_y_idx.idxmax(axis=1)\n",
    "    \n",
    "    return train_targets\n",
    "\n",
    "#### GET THE TRAINING DATAPOINT #####\n",
    "def Get_train_datas(train_x):\n",
    "    #### receive one hot data!!!! #####\n",
    "    one_hot_training_data = train_x\n",
    "    return one_hot_training_data\n",
    "\n",
    "class CustomDataset():\n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "\n",
    "        self.target = torch.LongTensor(labels)\n",
    "        self.data = torch.Tensor(np.asarray(data))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]\n",
    "    \n",
    "#### BATCH THE DATAPOINT ####\n",
    "def Get_batch(one_hot_training_data, train_targets, size):\n",
    "    # zip the data and target\n",
    "    training_data = CustomDataset(one_hot_training_data,train_targets)\n",
    "    # batch the data point\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size=size, shuffle=True)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        \n",
    "        super(Neural_Network, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(self.input_dim, 10)\n",
    "        \n",
    "        self.layer2 = torch.nn.Linear(10, self.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x.view(-1, self.input_dim))\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = Neural_Network(119, 119)\n",
    "optimizer = torch.optim.SGD(params=neural_net.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop:\n",
    "\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader, 1):\n",
    "        \n",
    "        # set optimizer gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        predictions = neural_net.forward(x)\n",
    "                \n",
    "        # backward pass + optimization step\n",
    "        loss = loss_fn(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f'Epoch: {epoch}, loss: {running_loss / i}')\n",
    "        \n",
    "    print(f'Loss after epoch: {epoch} = {running_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        outputs = neural_net(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks using PyTorch framework\n",
    "---\n",
    "![](resources/torch.png)\n",
    "## Comparing performance of classical networks to convolutional neural networks \n",
    "\n",
    "## Typical Deep Learning workflow:\n",
    "\n",
    "1. Load your training datasets, and(if needed) convert them into PyTorch datasets\n",
    "2. Build PyTorch-DataLoaders using your datasets, set shuffle = True and define batch size\n",
    "3. Define the neural network structure\n",
    "4. Training process:\n",
    "    - Define optimizer\n",
    "    - Define loss function\n",
    "    - Define # of training iterations\n",
    "    - Train your model\n",
    "5. Evaluation process:\n",
    "    - Use your model to predict labels for your test set\n",
    "    - evaluate accuracy with true labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Pair programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_data = torchvision.datasets.MNIST('data/', train=True, download=True, transform=transform)\n",
    "testing_data = torchvision.datasets.MNIST('data/', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data/\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data more closely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1eb861f158e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Look at a particular point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 1. plot it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Look at a particular point\n",
    "\n",
    "test_img = training_data.data[0]\n",
    "test_label = training_data.targets[0]\n",
    "# 1. plot it\n",
    "# 2. show it's format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloaders to feed data into our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20823941dc8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testing_data, batch_size=16, shuffle=True)\n",
    "#转换成torch可以处理的类型\n",
    "plt.imshow(list(train_loader)[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3750 * 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        \n",
    "        super(Conv_Net, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.Conv1 = torch.nn.Conv2d(1,6,5)\n",
    "        self.Conv2 = torch.nn.Conv2d(6,16,5)\n",
    "        \n",
    "        self.pool = torch.nn.MaxPool2d(2)\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(16 * 4 * 4, 50)\n",
    "        # 50 is the num of hidden neuro\n",
    "        self.layer2 = torch.nn.Linear(50, 100)\n",
    "        self.layer3 = torch.nn.Linear(100, self.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input shape: 28 x 28 x 6\n",
    "        x = self.Conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Input 24 x 24 x 6\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # 12 x 12 x 6\n",
    "        x = self.Conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 8 x 8 x 16\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # 4 x 4 x 16\n",
    "        # pssing the first layer\n",
    "        # x = self.layer1(x)\n",
    "        x = self.layer1(x.view(-1,16 * 4 * 4))\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # pssing the second layer\n",
    "        x = self.layer2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # pssing the third layer\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create an object of the neural network\n",
    "# 2. Define an optimizer\n",
    "# 3. Define a loss function\n",
    "\n",
    "network = Conv_Net(784, 10)\n",
    "optimizer = torch.optim.SGD(params = network.parameters(), lr = 0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error after epoch: 0 = 2.2960093760172526\n",
      "Error after epoch: 1 = 2.294060476620992\n",
      "Error after epoch: 2 = 2.2911027784347535\n",
      "Error after epoch: 3 = 2.2865263544718424\n",
      "Error after epoch: 4 = 2.27801403242747\n",
      "Error after epoch: 5 = 2.2610354305267335\n",
      "Error after epoch: 6 = 2.220646917025248\n",
      "Error after epoch: 7 = 2.1215331181526182\n",
      "Error after epoch: 8 = 1.9438045182863872\n",
      "Error after epoch: 9 = 1.7162078857421874\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "\n",
    "num_iterations = 10\n",
    "\n",
    "for epoch in range(num_iterations):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        x,y = data\n",
    "        \n",
    "        # Zero-out gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward pass\n",
    "        prediction = network.forward(x)\n",
    "        \n",
    "        #compute loss\n",
    "        loss = loss_fn(prediction, y)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #back propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Error after epoch: {epoch} = {running_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final acccuracy: 64.53% \n"
     ]
    }
   ],
   "source": [
    "# evaluation loop\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i, data in enumerate(test_loader):\n",
    "        \n",
    "        x,y = data\n",
    "        \n",
    "        predicted = network.forward(x)\n",
    "        \n",
    "        _, predicted = torch.max(predicted.data, 1)\n",
    "        \n",
    "        correct += (predicted == y).sum().item()\n",
    "        \n",
    "        total += y.size(0)\n",
    "        \n",
    "print(f'Final acccuracy: {correct / total * 100}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare regular Multilayer-perceptron performance against Convolutional neural network\n",
    "\n",
    "### How to compute output size after convolutional layer ??:\n",
    "- If stride = 1, padding = 0, dilation = 1 \n",
    "- output shape 0: (size(input.shape[0]) - kernel.shape[0]) + 1\n",
    "- same for output shape 1\n",
    "\n",
    "- Nice visualizations of different kernel/filter/convolution strategies: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "\n",
    "### How to compute output size after fully connected layer ??:\n",
    "- If stride = 1, padding = 0, dilation = 1 \n",
    "- output shape 0: (size(input.shape[0]) - kernel.shape[0]) + 1\n",
    "- same for output shape 1\n",
    "\n",
    "- Nice visualizations of different kernel/filter/convolution strategies: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create an object of the neural network\n",
    "# 2. Define optimizer\n",
    "# 3. Define a loss function\n",
    "\n",
    "neural_net = ConvNet()\n",
    "optimizer = torch.optim.SGD(params=neural_net.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Build your own Neural Network classifiers:\n",
    "\n",
    "### Todos:\n",
    "1. Load the CIFAR 10 train and test dataset from the torchvision library that we have used above for the MNIST data:\n",
    "Documentation: https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "2. Create DataLoaders for the training and test size:\n",
    "    - experiment with different batch sizes\n",
    "3. Create one fully connected model and another Convolutional Neural Network, for each experiment with different layer sizes(# of neurons) and layer types:\n",
    "    - Conv layers preprocess the data\n",
    "    - Pooling layers preprocess the data\n",
    "    - Fully connected layer need to be added at the end to classify the data\n",
    "\n",
    "4. Evaluate the prediction accuracy(all correct classified points / number of points) of your Fully-connected and Convolutional Neural Networks\n",
    "\n",
    "5. Evaluate prediction accuracy of each class, e.g.: Correctly classified: 60% of planes, 70% of cars, 30% of housed etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
